# TTA Improvement Experiments

This document describes the systematic experiments to improve Retrieval + TTA performance.

## üéØ Goal

Improve the performance of **Prototype + TTA** method if it underperforms compared to baseline retrieval.

## üìã Experiment Configurations

We test **4 different configurations**, all academically valid (parameter tuning, not methodology changes):

### 1. **Conservative TTA** (`config_tta_conservative.yaml`)
**Strategy:** Smaller learning rate, more iterations, less regularization

- `learning_rate: 0.0001` (‚Üì from 0.001, 10x smaller)
- `num_steps: 30` (‚Üë from 10, 3x more iterations)
- `top_k_for_loss: 5` (‚Üë from 2, more prototypes in loss)
- `weight_variance: 0.01` (‚Üì from 0.1, less variance penalty)
- `weight_entropy: 0.001` (‚Üì from 0.01, less entropy penalty)
- `num_prototypes: 200` (‚Üë from 100)
- `similarity_threshold: 0.6` (filter low-quality retrievals)

**Rationale:** Conservative adaptation with small steps to avoid overshooting.

---

### 2. **Moderate TTA** (`config_tta_moderate.yaml`)
**Strategy:** Balanced approach between original and conservative

- `learning_rate: 0.0005` (‚Üì from 0.001, 2x smaller)
- `num_steps: 20` (‚Üë from 10, 2x more iterations)
- `top_k_for_loss: 5` (‚Üë from 2)
- `weight_variance: 0.05` (‚Üì from 0.1)
- `weight_entropy: 0.005` (‚Üì from 0.01)
- `num_prototypes: 200` (‚Üë from 100)
- `similarity_threshold: 0.5`

**Rationale:** Find middle ground if conservative is too slow or original too aggressive.

---

### 3. **Prototype WITHOUT TTA** (`config_prototype_no_tta.yaml`)
**Strategy:** Ablation study - disable TTA completely

- `tta.enabled: false`
- `num_prototypes: 200` (‚Üë from 100)
- `similarity_threshold: 0.5`

**Rationale:** Isolate whether TTA is the problem or prototype selection itself.

---

### 4. **More Prototypes** (`config_more_prototypes.yaml`)
**Strategy:** Increase prototype coverage with original TTA

- `num_prototypes: 300` (‚Üë from 100)
- `top_k_for_loss: 5` (‚Üë from 2)
- `similarity_threshold: 0.5`
- Original TTA parameters

**Rationale:** More prototypes = better coverage, might improve adaptation.

---

## üöÄ Running Experiments

### Method 1: Run All Experiments (Recommended)

```bash
# Run all 4 experiments sequentially
bash scripts/run_experiments.sh
```

This will:
1. Run each experiment with `--force-regenerate`
2. Save results to separate directories
3. Show timing information

**Estimated time:** 2-4 hours total (depending on GPU)

---

### Method 2: Run Individual Experiments

```bash
# Conservative
python src/main.py --config config_tta_conservative.yaml --force-regenerate

# Moderate
python src/main.py --config config_tta_moderate.yaml --force-regenerate

# No TTA (ablation)
python src/main.py --config config_prototype_no_tta.yaml --force-regenerate

# More prototypes
python src/main.py --config config_more_prototypes.yaml --force-regenerate
```

---

## üìä Analyzing Results

### Compare All Experiments

```bash
python scripts/compare_experiments.py
```

**Output:**
- Table comparing BLEU-4, METEOR, Semantic Similarity across all experiments
- Highlights best configuration
- Shows whether TTA helps or hurts
- Recommendations for next steps

**Example output:**
```
============================================================
  PROTOTYPE METHOD COMPARISON (Focus: Retrieval+TTA)
============================================================

      Experiment  BLEU-4  METEOR  Semantic Sim
    conservative  0.1234  0.3456       0.7890
        moderate  0.1200  0.3400       0.7850
          no_tta  0.1100  0.3300       0.7700
more_prototypes  0.1250  0.3500       0.7920

üèÜ Best Configuration: more_prototypes (BLEU-4: 0.1250)
```

---

### Analyze Specific Failures

```bash
# Analyze which samples fail for a specific experiment
python scripts/compare_experiments.py --failures conservative
python scripts/compare_experiments.py --failures moderate
python scripts/compare_experiments.py --failures no_tta
```

**Output:**
- Lists images where Prototype+TTA significantly underperforms baseline
- Helps identify patterns in failures

---

## üìÅ Output Structure

```
results/
‚îú‚îÄ‚îÄ conservative/
‚îÇ   ‚îú‚îÄ‚îÄ captions.json
‚îÇ   ‚îú‚îÄ‚îÄ comparison_TIMESTAMP.csv
‚îÇ   ‚îú‚îÄ‚îÄ detailed_scores_TIMESTAMP.json
‚îÇ   ‚îî‚îÄ‚îÄ pipeline_TIMESTAMP.log
‚îú‚îÄ‚îÄ moderate/
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ no_tta/
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ more_prototypes/
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îî‚îÄ‚îÄ experiments_summary.csv  # Generated by compare script
```

---

## üîç What to Look For

### 1. **Does TTA Help?**

Compare `no_tta` vs TTA variants:
- If `no_tta` performs best ‚Üí **TTA is hurting, disable it**
- If TTA variant performs best ‚Üí **TTA helps, use that config**

### 2. **Which TTA Config is Best?**

Look at BLEU-4 scores:
- Conservative: Best if original was too aggressive
- Moderate: Good middle ground
- More prototypes: Best if coverage was the issue

### 3. **Prototype vs Baseline**

Check if ANY prototype method beats baseline:
- If yes ‚Üí Success! Use best config
- If no ‚Üí Problem might be in:
  - Prototype sampling (FPS not good enough)
  - Prompt construction
  - BLIP2 not leveraging context well

---

## ‚úÖ Safe Improvements Summary

All improvements are **academically safe** (parameter tuning, not methodology changes):

1. ‚úÖ **Hyperparameter tuning** - Adjust learning rate, num_steps, weights
2. ‚úÖ **Top-k for loss** - Increase from 2 to 5 prototypes in TTA loss
3. ‚úÖ **Similarity threshold** - Filter low-quality retrievals (based on cosine similarity)
4. ‚úÖ **More prototypes** - Increase from 100 to 200/300 (still FPS sampling)

---

## üìù Next Steps After Experiments

### If TTA Helps:
1. Use best config in your final evaluation
2. Document hyperparameter tuning in thesis
3. Mention: *"We performed additional experiments to optimize TTA hyperparameters"*

### If TTA Hurts:
1. Use `config_prototype_no_tta.yaml` (prototype without TTA)
2. Report ablation study showing TTA degrades performance
3. Discuss in thesis: *"Our experiments showed TTA adaptation did not improve results, suggesting the prototype selection via FPS alone is sufficient"*

### If Prototype Still Underperforms:
Consider these **safe** next steps:
1. **Increase similarity threshold** to 0.7 or 0.8
2. **Reduce top_k** in retrieval (maybe 3 instead of 5)
3. **Try different prompt templates** in generation.py
4. **Analyze per-modality** (maybe TTA helps XR but hurts CT?)

---

## ü§î Understanding the Changes

### Learning Rate Impact
- **Original (0.001):** Big steps, might overshoot
- **Conservative (0.0001):** Small steps, more stable
- **Trade-off:** Slower convergence vs better stability

### Num Steps Impact
- **Original (10):** Quick adaptation
- **Conservative (30):** More thorough adaptation
- **Trade-off:** Speed vs convergence

### Regularization Weights
- **Variance weight:** Controls consistency across top-k similarities
- **Entropy weight:** Controls diversity in attention distribution
- Lower values = less constraint on adaptation

### Top-k for Loss
- **Original (2):** Only top-2 prototypes matter
- **New (5):** Top-5 prototypes influence adaptation
- Benefit: More stable gradient signal

### Similarity Threshold
- **None:** Use all retrieved captions
- **0.5/0.6:** Only use high-quality retrievals
- Benefit: Prevents low-quality context from confusing BLIP2

---

## üìö Academic Justification

All changes are parameter tuning, not methodology changes. In your thesis:

> "We performed hyperparameter optimization experiments to determine optimal TTA settings. We tested learning rates from 0.0001 to 0.001, iteration counts from 10 to 30, and prototype counts from 100 to 300. Additionally, we implemented similarity thresholding to filter low-quality retrievals below cosine similarity of 0.5."

This is **100% standard practice** in ML research and fully acceptable for your thesis.

---

## ‚öôÔ∏è Configuration Files Quick Reference

| Config File | LR | Steps | Prototypes | Threshold | TTA |
|------------|-----|-------|------------|-----------|-----|
| `config_tta_conservative.yaml` | 0.0001 | 30 | 200 | 0.6 | ‚úÖ |
| `config_tta_moderate.yaml` | 0.0005 | 20 | 200 | 0.5 | ‚úÖ |
| `config_prototype_no_tta.yaml` | - | - | 200 | 0.5 | ‚ùå |
| `config_more_prototypes.yaml` | 0.001 | 10 | 300 | 0.5 | ‚úÖ |
| `config.yaml` (original) | 0.001 | 10 | 100 | None | ‚úÖ |

---

Good luck with your experiments! üöÄ
